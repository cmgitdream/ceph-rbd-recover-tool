# author: min chen(minchen@ubuntukylin.com) 2014 2015


------------- ceph rbd recovery tool -------------

  ceph rbd recover tool is used for recovering ceph rbd image, when all ceph services are killed.
it is based on ceph-0.8x.y (Firefly and upper)
  currently, ceph service(ceph-mon, ceph-osd) evently are not avaiable caused by bugs
, especially on large scale ceph cluster, then rbd images can not be accessed.
ceph rbd recover tool is designed to solve this problem.
  but, there are some limitions:
-osdmap is not changed, no pg migration
-pg epoch is sync with object content
-osd journals are flushed to disks

before you run this tool, you should make sure that:
1). all processes (ceph-osd, ceph-mon, ceph-mds) are shutdown
2). ssh deamon is running & network is ok (ssh to each node without password)
3). ceph-kvstore-tool is installed(for ubuntu: apt-get install ceph-test)
4). osd disk is not crashed and data can be accessed on local filesystem

-architecture:
  admin_job		osd_job
                      +---- osd.0
                      |
admin_node -----------+---- osd.1
                      |
                      +---- osd.2
		      |
                      ......

-files:
admin_node: {admin_job  common_h  epoch_h  metadata_h  database_h}
osd:        {osd_job    common_h  epoch_h  metadata_h} #/var/osd_job


-config file
osd_host_path: osd hostnames and osd data path #user input
  osdhost0	/var/lib/ceph/osd/ceph-0
  osdhost1	/var/lib/ceph/osd/ceph-1
  ......
mon_host: all mon node hostname #user input
  monhost0
  monhost1
  ......
mds_host: all mds node hostname #user input
  mdshost0
  mdshost1
  ......
then, init_env_admin function will create file: osd_host
osd_host: all osd node hostname #generated by admin_job, user ignore it
  osdhost0
  osdhost1
  ......
you must input config files before run this tool,
if you want to refresh config files:
modify: osd_host_path, mon_host, mds_host
remove: osd_host

-usage:
./admin_job <operation>
<operation> :
database		#generating offline database: hobject path, node hostname, pg_epoch and image metadata
list			#list all images from offline database
lookup <image_name>	#lookup image metadata in offline database
recover <image_name> [/path/to/store/image]	#recover image data according to image metadata

-steps:
1. stop all ceph services: ceph-mon, ceph-osd, ceph-mds
2. setup config files: osd_host_path, mon_host, mds_host
3. ./admin_job database 	# wait a long time 
4. ./admin_job recover <image_name> [/path/to/store/image]


-debug & error check
if admin_node operation is failed, you can check it on osd node
# cd /var/osd_job
# ./osd_job <operation>
<opeartion> :
do_image_id <image_id_hobject>		#get image id of image format v2 
do_image_id <image_header_hobject>	#get image id of image format v1
do_image_metadata_v1 <image_name>  	#get image metadata of image format v1, maybe pg epoch is not latest
do_image_metadata_v2 <image_name>  	#get image metadata of image format v2, maybe pg epoch is not latest
do_image_list 				#get all images on this osd(image head hobject)
do_pg_epoch				#get all pg epoch and store it in /var/single_node/node_pg_epoch
do_omap_list    			#list all omap headers and omap entries on this osd
